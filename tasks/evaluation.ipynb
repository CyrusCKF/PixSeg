{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "from timeit import default_timer\n",
    "\n",
    "import torch\n",
    "import tqdm\n",
    "from PIL import Image\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import VOCSegmentation\n",
    "from torchvision.models.segmentation import fcn_resnet50\n",
    "from torchvision.transforms.v2 import functional as TF\n",
    "\n",
    "sys.path.append(str(Path(\"..\").resolve()))\n",
    "from src.semantic_segmentation_toolkit.datasets import (\n",
    "    DATASET_ZOO,\n",
    "    CityscapesClass,\n",
    "    resolve_metadata,\n",
    ")\n",
    "from src.semantic_segmentation_toolkit.models import FCN_ResNet34_Weights, fcn_resnet34\n",
    "from src.semantic_segmentation_toolkit.pipeline import (\n",
    "    TesttimeAugmentations,\n",
    "    create_snapshots,\n",
    "    eval_one_epoch,\n",
    "    inference_with_augmentations,\n",
    "    inference_with_sliding_window,\n",
    ")\n",
    "from src.semantic_segmentation_toolkit.utils.metrics import MetricStore\n",
    "from src.semantic_segmentation_toolkit.utils.rng import seed\n",
    "from src.semantic_segmentation_toolkit.utils.transform import (\n",
    "    SegmentationAugment,\n",
    "    SegmentationTransform,\n",
    ")\n",
    "from src.semantic_segmentation_toolkit.utils.visual import combine_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = resolve_metadata(\"Cityscapes\")\n",
    "transforms = SegmentationTransform(mask_fill=metadata.ignore_index)\n",
    "augment = SegmentationAugment(mask_fill=metadata.ignore_index)\n",
    "dataset = CityscapesClass(\n",
    "    root=r\"..\\..\\Datasets\\Cityscapes\",\n",
    "    split=\"val\",\n",
    "    target_type=\"semantic\",\n",
    "    transforms=transforms,\n",
    ")\n",
    "data_loader = DataLoader(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_state_file = Path(r\"..\\runs\\20250227_171651\\latest_model.pth\")\n",
    "model = fcn_resnet50(num_classes=metadata.num_classes, aux_loss=True)\n",
    "model_weights = torch.load(model_state_file)\n",
    "model.load_state_dict(model_weights)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=metadata.ignore_index)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(device).eval()\n",
    "criterion.to(device).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snapshots = create_snapshots(model, dataset, augment, device, metadata.colors)\n",
    "combined = combine_images([s for ss in snapshots for s in ss])\n",
    "combined_pil: Image.Image = TF.to_pil_image(combined)\n",
    "combined_pil.reduce(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Without augmentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed(42)\n",
    "ms = eval_one_epoch(\n",
    "    model, data_loader, augment, criterion, device, metadata.num_classes\n",
    ")\n",
    "pprint(ms.summarize())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With augmentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ttas = TesttimeAugmentations(\n",
    "    (1,), (False, True), (False,), (0,), iter_product=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed(42)\n",
    "ms = MetricStore(metadata.num_classes)\n",
    "loader = tqdm.tqdm(iter(data_loader), total=len(data_loader))\n",
    "for images, masks in loader:\n",
    "    start_time = default_timer()\n",
    "    prelim_images, masks = augment(images.to(device), masks.to(device))\n",
    "    augmented_logits = inference_with_augmentations(model, prelim_images, ttas)\n",
    "\n",
    "    # may change how to aggregate results\n",
    "    aggregated_logits = torch.mean(augmented_logits, dim=0)\n",
    "    loss = criterion(aggregated_logits, masks)\n",
    "    end_time = default_timer()\n",
    "\n",
    "    preds = aggregated_logits.argmax(1)\n",
    "    ms.store_results(masks, preds)\n",
    "    batch_size = images.size(0)\n",
    "    measures = {\n",
    "        \"loss\": loss.item() * batch_size,\n",
    "        \"time\": end_time - start_time,\n",
    "    }\n",
    "    ms.store_measures(batch_size, measures)\n",
    "    loader.set_postfix(ms.summarize())\n",
    "pprint(ms.summarize())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With sliding window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = (512, 1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed(42)\n",
    "ms = MetricStore(metadata.num_classes)\n",
    "loader = tqdm.tqdm(iter(data_loader), total=len(data_loader))\n",
    "for images, masks in loader:\n",
    "    start_time = default_timer()\n",
    "    prelim_images, masks = augment(images.to(device), masks.to(device))\n",
    "    augmented_logits = inference_with_sliding_window(model, prelim_images, window_size)\n",
    "    aggregated_logits = torch.mean(augmented_logits, dim=0)\n",
    "    end_time = default_timer()\n",
    "\n",
    "    preds = aggregated_logits.argmax(1)\n",
    "    ms.store_results(masks, preds)\n",
    "    batch_size = images.size(0)\n",
    "    measures = {\"time\": end_time - start_time}\n",
    "    ms.store_measures(batch_size, measures)\n",
    "    loader.set_postfix(ms.summarize())\n",
    "pprint(ms.summarize())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For submitting to cityscapes benchmark suite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_folder = Path(r\"cityscapes_semantic\")\n",
    "out_folder.mkdir(parents=True)\n",
    "full_metadata = resolve_metadata(\"CityscapesFull\")\n",
    "\n",
    "ttas = TesttimeAugmentations((1,), (False, True), (False,), (0,), iter_product=True)\n",
    "loader = tqdm.tqdm(enumerate(data_loader), total=len(data_loader))\n",
    "for i, (images, masks) in loader:\n",
    "    prelim_images, masks = augment(images.to(device), masks.to(device))\n",
    "    augmented_logits = inference_with_augmentations(model, prelim_images, ttas)\n",
    "    aggregated_logits = torch.mean(augmented_logits, dim=0)\n",
    "\n",
    "    # convert preds to labelIDs\n",
    "    preds = aggregated_logits.argmax(1)\n",
    "    label_id_pred = torch.zeros_like(preds, dtype=torch.uint8)\n",
    "    for train_id, train_label in enumerate(metadata.labels):\n",
    "        if train_label in full_metadata.labels:\n",
    "            label_id = full_metadata.labels.index(train_label)\n",
    "            label_id_pred[preds == train_id] = label_id\n",
    "\n",
    "    image_path = Path(dataset.images[i])\n",
    "    preds_pil: Image.Image = TF.to_pil_image(label_id_pred)\n",
    "    palette = [c for rgb in full_metadata.colors for c in rgb]\n",
    "    preds_pil.putpalette(palette)\n",
    "    preds_pil.save(out_folder / image_path.with_suffix(\".png\").name)\n",
    "\n",
    "shutil.make_archive(\"cityscapes_semantic\", \"zip\", out_folder)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
